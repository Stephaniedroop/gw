expected_entropy <- function(choice,p_hyps) {
pwin <- expected_outcome(choice,p_hyps)
cat_entropy(pwin)
}
print(expected_entropy(2,post))
#For steph 2
library(igraph)
library(tidyverse)
nodes<-c('Preference','Character','Closer','Knowledge','Visible','Choice')
graph<-matrix(c(0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,0), ncol = 6, byrow=T)
G<-graph.adjacency(graph)
View(G)
V(G)$label<-V(G)$name<-nodes
V(G)$size <- 50
V(G)$color <- 'white'
E(G)$color <- 'black'
E(G)$width <- 2
plot(G)
?igraph
V(G)$color <- 'blue'
E(G)$color <- 'black'
E(G)$width <- 2
plot(G)
V(G)$color <- 'white'
E(G)$color <- 'red'
E(G)$width <- 2
plot(G)
#I figured its easier to think about in the form of a data frame
#So we have a column indexing the state for each cause using a factor (0,1)
#And a column for the probabilities for each unique combination
pChoice<-data.frame(expand.grid(list(Preference=c(0,1),
Character=c(0,1),
Closer=c(0,1),
Knowledge=c(0,1),
Visible = c(0,1)))) %>%
mutate(Preference = factor(Preference, levels = c(0,1),
labels = c('Absent','Hotdog')),
Character = factor(Character, levels = c(0,1),
labels = c('Lazy','Sporty')),
Closer = factor(Closer, levels = c(0,1),
labels = c('Pizza','Hotdog')),
Knowledge = factor(Knowledge, levels = c(0,1),
labels = c('No','Yes')),
Visible = factor(Visible, levels = c(0,1),
labels = c('Pizza','Hotdog')),
p_choose_hotdog = NA)
View(pChoice)
head(pChoice)
#Default preference for hotdog
baserate<-.5
#Strength of preference, strength of character
strengths<-list(preference=.5, character=.5)
#preference and character match only promote hotdog choice so long as the person either
#(a) knows the area or (b) can see the hotdog
tmp1<- pChoice$Knowledge=='Yes' | pChoice$Visible=='Hotdog'
#character match only promotes pizza choice so long as the person
#(a) knows the area or (b) can see the hotdog
tmp2<-pChoice$Knowledge=='Yes' | pChoice$Visible=='Pizza'
pChoice$p_choose_hotdog<-(1-(1-baserate) *
(1-strengths[['preference']]*as.numeric(pChoice$Preference=='Hotdog')*tmp1) *
#preference for hotdog pushes toward hotdog
(1-strengths[['character']]* as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Hotdog' |
#character match to distance to hotdog pushes toward hotdog
pChoice$Character=='Sporty' & pChoice$Closer=='Pizza')*tmp1)) *
#end of the noisy OR of generative causes
(1-strengths[['character']] * as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Pizza' |
#character match to distance to pizza pushes toward pizza
pChoice$Character=='Sporty' & pChoice$Closer=='Hotdog') * tmp2)
View(pChoice)
#Here's how it looks
pChoice
#Anc here's an example of using it to maker a basic counterfactual explanation type model
#Let's suppose a person chooses pizza in situation 15
case<-cbind(pChoice[15,], Choice='Pizza')
case
#Was it reasonably to be expected given the situation (in this case yes)
p_actual<-1-case$p_choose_hotdog
?unlist
#What features of the situation contributed most to its being selected?
#counterfactual contrasts:
p_counterfactual<-1-unlist(c(pChoice %>% filter(Preference!=case$Preference,
Character==case$Character,
Closer ==case$Closer,
Knowledge==case$Knowledge,
Visible==case$Visible) %>% select(preference=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character!=case$Character,
Closer ==case$Closer,
Knowledge==case$Knowledge,
Visible==case$Visible) %>% select(character=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character==case$Character,
Closer !=case$Closer,
Knowledge==case$Knowledge,
Visible==case$Visible) %>% select(closer=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character==case$Character,
Closer ==case$Closer,
Knowledge!=case$Knowledge,
Visible==case$Visible) %>% select(knowledge=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character==case$Character,
Closer ==case$Closer,
Knowledge==case$Knowledge,
Visible!=case$Visible) %>% select(visible=p_choose_hotdog)))
View(case)
dependence<-p_actual-p_counterfactual
#In this case we might reasonably blame her lazy character or the fact that the pizza was closer
Collapse
View(case)
View(pChoice)
dependence
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_push * char_push
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
pref_match_hotdog <- as.numeric(pChoice$Preference=='Hotdog')
char_match_hotdog <- as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Hotdog' |
pChoice$Character=='Sporty' & pChoice$Closer=='Pizza')
char_match_pizza <- as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Pizza' |
#character match to distance to pizza pushes toward pizza
pChoice$Character=='Sporty' & pChoice$Closer=='Hotdog')
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
#preference and character match only promote hotdog choice so long as the person either
#(a) knows the area or (b) can see the hotdog ie generative ie things that make you want hotdog
prom_hotdog <- pChoice$Knowledge=='Yes' | pChoice$Visible=='Hotdog'
#character match only promotes pizza choice so long as the person
#(a) knows the area or (b) can see the hotdog ie preventative factor ie things that amke you want pizza
prom_pizza <- pChoice$Knowledge=='Yes' | pChoice$Visible=='Pizza'
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_push * char_push
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(strengths[['character']], strengths[['preference']], baserate))
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate))
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate))
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate)))
# My meddling
hotdog_promotion <- function() {
pref_prom_hotdog <- 1 - strengths[['preference']] * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - strengths[['character']] * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion())
# My meddling
hotdog_promotion <- function() {
pref_prom_hotdog <- 1 - strengths[['preference']] * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - strengths[['character']] * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1- strengths[['character']] *char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion())
22/110
2.002/10.10
5.005/10.01
2.005/10.1
2.002/10.1
52/110
exp(100)
exp(1)
runif(4)
sessionInfo()
sessionInfo()
sessionInfo()
install.packages("afex")
install.packages("faux")
library(broom, tidyverse, faux, afex)
library(broom)
library(faux)
library(afex)
R.version()
R.Version()
.libPaths()
.libPaths()
R.Version()
# ------- Prelims -----------
library(tidyverse)
library(ggplot2)
# ----------- Define an example prior df -------------------------
# Here define two causal vars and an exogenous noise variable for each (i.e. var epsilon A goes with A)
# in the exp setting this is 0.5
p_A <- c(.1,.9) # ie A usually has value 1... base rate for cause
p_epsA <- c(.7,.3) #... most of the time the noise var for a doesn't occur. for a to work it needs a and exp a. a is usually present but ogten doesnt work cos of noise term not working
p_B <- c(.8,.2) # B rarely fires 1...
p_epsB <- c(.3,.7) # but when it does it is strong
# And wrap them into a df called prior. Later the function should take dfs of this format:
# i.e. any number of causes as the rows, and the probs of them taking 0 and 1 as cols
params <- data.frame(rbind(p_A, p_epsA, p_B, p_epsB))
colnames(params) <- c(0,1)
# Other values set outside for now
N_cf <- 1000L # How many counterfactual samples to draw
s <- .7 # Stability
n_causes <- nrow(params)
causes <- rownames(params)
# Make a df of all combinations of variable settings
df <- expand.grid(rep(list(c(0,1)),n_causes), KEEP.OUT.ATTRS = F)
# ... with variables as the column names
colnames(df) <- causes
worlds <- nrow(df)
View(df)
structure <- 'disjunctive'
if (structure=="disjunctive") {
df$E <- as.numeric((df[1] & df[2]) | (df[3] & df[4]))
}
# Can replace with this - if rename - it is deterministic - literally gives specific outcome for set 3 causes, needs actual input. mechanical tell syou whether effects occurred given setting
# df$effect <- max( c(min(c1,e1), min(c2,e2), min(c3, e3), min(c2*c3, e23))) # BUT SAME PROBLEM - HOW TO AUTOMATICALLY DEAL WITH ANY NUMBER OF CAUSES?
mat <- as.matrix(df[,1:4])
View(mat)
# df2 <- as.matrix(df, dimnames=NULL)
# dimnames = list(c(1:16), c(causes))
# Replace every cell with the relevant indexed edge strength from params
for (k in 1:worlds){
for (cause in causes) {
a <- params[cause,df[k,cause]+1] # It needs the '+1' because r indexes from 1 not 0
mat[k,cause] <- a # ((then sometimes #*df[k,cause] if do at same time as structure but change later if need))
}
}
View(mat)
View(params)
# For each row of df, the prior is now the product of the same row of df2
df$Pr <- apply(mat, 1, prod) # parameter of the model
sum(df$Pr)
# Then loop to calculate cfs and assign causal responsibility
# Loop through possible world settings
for (c_ix in 1:worlds)
setwd("/Users/stephaniedroop/Documents/GitHub/gw/Collider/Main_scripts")
all <- read.csv('../model_data/all.csv') %>% replace(is.na(.), 0) # 4032 of 19.
library(tidyverse)
all <- read.csv('../model_data/all.csv') %>% replace(is.na(.), 0) # 4032 of 19.
View(all)
# 2016 of 12
model_preds <- all %>% select(-c(V5:V9, structure.y, pgroup.y)) %>%
rename(structure = structure.x,
pgroup = pgroup.x,
A = pA.x,
Au = peA.x,
B = pB.x,
Bu = peB.x) %>%
group_by(s, pgroup, structure, index) %>% summarise(meanA = mean(A),
sdA = sd(A),
meanAu = mean(Au),
sdAu = sd(Au),
meanB = mean(Bu),
sdB = sd(B),
meanBu = mean(Bu),
sdBu = sd(Bu), na.rm=T)
View(model_preds)
model_preds2 <- as.data.frame(model_preds)
ss <- model_preds2 %>% filter(s==0.00)
View(ss)
ss <- model_preds2 %>% filter(s==0.00, pgroup==1)
View(ss)
sss <- all %>% filter(s==0, pgroup.x==1, structure.x=='disjunctive')
View(sss)
load('../model_data/params.rdata', verbose = T) # defined in script `set_params.r`
sss <- all %>% filter(s==0, pgroup.x==1, structure.x=='disjunctive', index==1)
View(sss)
var(sss$V5)
sd(sss$V5)
sd(sss$V8)
sd(sss$V7)
all <- all %>% rename(A_cp = 3, Au_cp = 4, B_cp = 5, Bu_cp = 6,
structure = 12, pgroup = 15,
vA = 16, vAu = 17, vB = 18, vBu = 19)
all$pgroup <- as.factor(all$pgroup)
# Bring in trialtype and rename as the proper string name just in case
all$trialtype <- all$group
all$trialtype[all$trialtype==1 & all$structure=='disjunctive'] <- 'd1'
all$trialtype[all$trialtype==2 & all$structure=='disjunctive'] <- 'd2'
all$trialtype[all$trialtype==3 & all$structure=='disjunctive'] <- 'd3'
all$trialtype[all$trialtype==4 & all$structure=='disjunctive'] <- 'd4'
all$trialtype[all$trialtype==5 & all$structure=='disjunctive'] <- 'd5'
all$trialtype[all$trialtype==6 & all$structure=='disjunctive'] <- 'd6'
all$trialtype[all$trialtype==7 & all$structure=='disjunctive'] <- 'd7'
all$trialtype[all$trialtype==1 & all$structure=='conjunctive'] <- 'c1'
all$trialtype[all$trialtype==2 & all$structure=='conjunctive'] <- 'c2'
all$trialtype[all$trialtype==3 & all$structure=='conjunctive'] <- 'c3'
all$trialtype[all$trialtype==4 & all$structure=='conjunctive'] <- 'c4'
all$trialtype[all$trialtype==5 & all$structure=='conjunctive'] <- 'c5'
all$A_cp[all$vA!=all$E] <- 0
all$Au_cp[all$vAu!=all$E] <- 0
all$B_cp[all$vB!=all$E] <- 0
all$Bu_cp[all$vBu!=all$E] <- 0
all <-  all %>% select(-c(X, structure.y, pgroup.y))
# Multiply raw effect sizes by cond.prob for what will make up what we're calling 'weighted average'
all[,24:27] <- all[,2:5]*all$cond
# Multiply raw effect sizes by cond.prob for what will make up what we're calling 'weighted average'
all[,24:27] <- all[,2:5]*all$cond %>% rename(A_wa = 24, Au_wa = 25, B_wa = 26, Bu_wa = 27)
all <- all %>% rename(A_wa = 24, Au_wa = 25, B_wa = 26, Bu_wa = 27)
# This is the step we were preparing for!
# cp is the conditional cesm, and wa is that * con.probs
# Now we want to structure it slightly longer, on node only, matching the 'cp' and the 'wa' to node
all <- all %>% pivot_longer(cols = -c(index, V5:trialtype), names_to = c('node', '.value'),
names_sep = '_') # Gives 768 of 15 vars (126 for each of 6 probgroups)
all$node2 <- all$node
all$node[all$vAu=='0' & all$node2=="Au"] <- 'Au=0'
all$node[all$vAu=='1' & all$node2=="Au"] <- 'Au=1'
all$node[all$vBu=='0' & all$node2=="Bu"] <- 'Bu=0'
all$node[all$vBu=='1' & all$node2=="Bu"] <- 'Bu=1'
# Also need one with 8, where every node takes the value it has
all$node3 <- all$node
all$node3[all$vA=='0' & all$node2=='A'] <- 'A=0'
all$node3[all$vA=='1' & all$node2=='A'] <- 'A=1'
all$node3[all$vB=='0' & all$node2=='B'] <- 'B=0'
all$node3[all$vB=='1' & all$node2=='B'] <- 'B=1'
# Later we may delete this line and do the 0s elsewhere -- 16320 / 16128
all <- all %>% complete(pgroup, trialtype, node3, s) # 20160 - problem because lots of NAs
# Try filling the blank data
# Strcuture
all$structure <- if_else(grepl("^c", all$trialtype), 'conjunctive', 'disjunctive')
all$wa <- all$wa %>% replace(is.na(.), 0) # THINK WE'RE OK NOW????
# Get a tag of the unobserved variables' settings. Then we can group data by this for plotting
all <- all %>% unite("uAuB", vAu,vBu, sep= "", remove = FALSE)
all$grp <- all$group
all$grp[all$grp=='1'] <- 'A=0, B=0, | E=0'
all$grp[all$grp=='2'] <- 'A=0, B=1, | E=0'
all$grp[all$grp=='3' & all$structure=='disjunctive'] <- 'A=0, B=1, | E=1'
all$grp[all$grp=='3' & all$structure=='conjunctive'] <- 'A=1, B=0, | E=0'
all$grp[all$grp=='4' & all$structure=='disjunctive'] <- 'A=1, B=0, | E=0'
all$grp[all$grp=='4' & all$structure=='conjunctive'] <- 'A=1, B=1, | E=0'
all$grp[all$grp=='5' & all$structure=='disjunctive'] <- 'A=1, B=0, | E=1'
all$grp[all$grp=='5' & all$structure=='conjunctive'] <- 'A=1, B=1, | E=1'
all$grp[all$grp=='6'] <- 'A=1, B=1, | E=0'
all$grp[all$grp=='7'] <- 'A=1, B=1, | E=1'
# And same for the unobserved values only
all$uAuB2 <- all$uAuB
all$uAuB2[all$uAuB2=='00'] <- 'Au=0, Bu=0'
all$uAuB2[all$uAuB2=='01'] <- 'Au=0, Bu=1'
all$uAuB2[all$uAuB2=='10'] <- 'Au=1, Bu=0'
all$uAuB2[all$uAuB2=='11'] <- 'Au=1, Bu=1'
# We can also add a column called isLat for just whether the node is latent (Au,Bu) or observed (A,B).
all <- all %>% mutate(isLat = if_else(grepl(c("^Au|^Bu"), node3), 'TRUE', 'FALSE'))
# And one for whether the node is connected with A or B
all <- all %>% mutate(connectedWith = ifelse(node3=='A=0'|node3=='A=1'|node3=='Au=0'|node3=='Au=1', 'A', 'B'))
# But there is another more nuanced quality: realLatent...
# Sometimes the values of the unobserved variables can be inferred logically. These are NOT 'realLatent'.
# realLatent is when we genuinely don't know what values the unobserved variables take. (when poss >1 in the function `get_cond_probs`)
# It affects the following situations (easier to point out when it is NOT realLatent, and take the inverse)
# All unobserved are realLatent, except:
# c5: Au and Bu
# d2: Bu
# d3: Bu
# d4: Au
# d5: Au
# d6: Au and Bu
# Now encode those rules, putting FALSE. (Everything else is already correctly determined)
all$realLat <- all$isLat
all$realLat[all$trialtype=='c5'|all$trialtype=='d6'] <- FALSE
all$realLat[all$trialtype=='d2' & all$node2=='Bu'] <- FALSE
all$realLat[all$trialtype=='d3' & all$node2=='Bu'] <- FALSE
all$realLat[all$trialtype=='d4' & all$node2=='Au'] <- FALSE
all$realLat[all$trialtype=='d5' & all$node2=='Au'] <- FALSE
# write this as csv in case need it later
write.csv(all, '../model_data/tidied_preds2.csv')
load('../Data/Data.Rdata', verbose = T) # This is one big df, 'data', 3408 obs of 18 ie. 284 ppts
mod <- read.csv('../model_data/tidied_preds2.csv')
mod$s <- as.character(mod$s)
mp <- mod[mod$s %in% params$stab, ]
mp <- read.csv('../model_data/tidied_preds2.csv') # 82656 of 31
mp$pgroup <- as.factor(mp$pgroup)
mp$node3 <- as.factor(mp$node3)
mp$trialtype <- as.factor(mp$trialtype)
modelNorm <- mp %>% # 288 of 5
group_by(pgroup, trialtype, node3, .drop = FALSE) %>%
summarise(predicted = sum(wa)) %>%
mutate(normpred = predicted/sum(predicted))
View(modelNorm)
# An odd little way to bring in realLat status, must be a better way
modelPlaceholder <- mp %>%
na.omit() %>%
group_by(pgroup, trialtype, node3, realLat, .drop = FALSE) %>%
tally
# For realLat, everything TRUE has been defined as such.
# Everything else needs to be FALSE (currently some are na because we used .drop=F to get all the combinations).
modelPlaceholder <- modelPlaceholder %>% replace(is.na(.), FALSE) %>% select(-n)
modelNorm <- merge(modelNorm, modelPlaceholder)
View(modelNorm)
# First set factors so we can use tally
data$pgroup <- as.factor(data$pgroup)
data$node3 <- as.factor(data$node3)
data$trialtype <- as.factor(data$trialtype)
data$isPerm <- as.factor(data$isPerm)
# Bring in the isPerm status of all the node answers actually given
dataPlaceholder <- data %>% # 214
na.omit() %>%
group_by(pgroup, trialtype, node3, isPerm) %>% # prob no need for .drop because the isPerm val is the same for both values of the var
tally
dataNorm <- data %>% # 289
group_by(pgroup, trialtype, node3, .drop=FALSE) %>% # Here we do need the .drop to get all the combinations
tally %>%
mutate(prop=n/sum(n))
# Merge and keep all the 288 combinations, adding the isPerm vals for the ones that got an answer
dataNorm <- merge(x=dataNorm, y=dataPlaceholder, all.x = T) %>% replace(is.na(.), FALSE) %>% select(1,2,3,6,4,5)
# ----------- 3. The actual merge! ------------
# 576 of 7 -- CLOSEST THING TO SINGLE PREDICTION PER NODE/EXPLANATION
combNorm <- merge(x=dataNorm, y=modelNorm) %>%
select(-predicted, -n) %>%
rename(cesm=normpred, ppts=prop) %>%
pivot_longer(cols = cesm:ppts, values_to = 'percent')
# One for the scatter plot
combNorm2 <- merge(x=dataNorm, y=modelNorm) %>%
select(-predicted, -n) %>%
rename(cesm=normpred, ppts=prop) %>%
filter(isPerm==TRUE)
# For by-facet correlations
combNorm3 <- merge(x=dataNorm, y=modelNorm) %>%
select(-predicted, -n) %>%
rename(cesm=normpred, ppts=prop)
View(combNorm3)
library(ggnewscale)
#library(shiny)
rm(list=ls())
s_vals <- seq(0.00, 1.00, 0.05)
# Generate series of RMarkdown reports with plots to assess model fit, for each value of s separately
lapply(unique(s_vals), function(stab.i) {
rmarkdown::render(input = 'combine_per_s.Rmd',
params = list(stab = stab.i),
output_file = paste0(stab.i, '.html'))
})
# Generate series of RMarkdown reports with plots to assess model fit, for each value of s separately
lapply(unique(s_vals), function(stab.i) {
rmarkdown::render(input = 'combine_per_s.Rmd',
params = list(stab = stab.i),
output_file = paste0(stab.i, '.html'))
})
rm(list=ls())
load('../model_data/params.rdata', verbose = T) # defined in script `set_params.r`
# Load functions: world_combos, get_cond_probs, generic_cesm
source('functions.R')
# Empty df to put everything in
all <- data.frame(matrix(ncol=18, nrow = 0))
i <- 1
# 1) Get possible world combos of two observed variables in both dis and conj structures
dfd <- world_combos(params = poss_params[[i]], structure = 'disjunctive')
View(dfd)
sum(dfd$Pr)
i <- 2
# 1) Get possible world combos of two observed variables in both dis and conj structures
dfd <- world_combos(params = poss_params[[i]], structure = 'disjunctive')
View(dfd)
sum(dfd$Pr)
rm(list=ls())
all <- read.csv('../model_data/all.csv') %>% replace(is.na(.), 0) # 4032 of 19. now 20160 of 25
View(all)
all %>% filter(group==1, structure.x=='disjunctive')
all %>% filter(group==1, structure.x=='disjunctive', pgroup.x==2)
all %>% filter(group==1, structure.x=='disjunctive', pgroup.x==2,s==0.8)
all <- all %>% rename(A_cp = 3, Au_cp = 4, B_cp = 5, Bu_cp = 6,
structure = 12, pgroup = 15,
vA = 16, vAu = 17, vB = 18, vBu = 19)
all$pgroup <- as.factor(all$pgroup)
all <-  all %>% select(-c(X, structure.y, pgroup.y))
# Bring in trialtype and rename as the proper string name just in case
all$trialtype <- all$group
all$trialtype[all$trialtype==1 & all$structure=='disjunctive'] <- 'd1'
all$trialtype[all$trialtype==2 & all$structure=='disjunctive'] <- 'd2'
all$trialtype[all$trialtype==3 & all$structure=='disjunctive'] <- 'd3'
all$trialtype[all$trialtype==4 & all$structure=='disjunctive'] <- 'd4'
all$trialtype[all$trialtype==5 & all$structure=='disjunctive'] <- 'd5'
all$trialtype[all$trialtype==6 & all$structure=='disjunctive'] <- 'd6'
all$trialtype[all$trialtype==7 & all$structure=='disjunctive'] <- 'd7'
all$trialtype[all$trialtype==1 & all$structure=='conjunctive'] <- 'c1'
all$trialtype[all$trialtype==2 & all$structure=='conjunctive'] <- 'c2'
all$trialtype[all$trialtype==3 & all$structure=='conjunctive'] <- 'c3'
all$trialtype[all$trialtype==4 & all$structure=='conjunctive'] <- 'c4'
all$trialtype[all$trialtype==5 & all$structure=='conjunctive'] <- 'c5'
all$A_cp[all$vA!=all$E] <- 0
all$Au_cp[all$vAu!=all$E] <- 0
all$B_cp[all$vB!=all$E] <- 0
all$Bu_cp[all$vBu!=all$E] <- 0
# Multiply raw effect sizes by cond.prob for what will make up what we're calling 'weighted average',
# and rename to follow same pattern as _cp, so we can pivot by what kind of model prediction it is
all[,24:27] <- all[,2:5]*all$cond
all <- all %>% rename(A_wa = 24, Au_wa = 25, B_wa = 26, Bu_wa = 27)
all %>% filter(s==0.8, trialtype=='d1', pgroup==2)
