unnp <- likes*prior
z <- sum(unnp)
unnp/z
}
post <- update_probs(prior,choice=1,outcome=0)
print(post)
update_probs <- function(prior,choice,outcome) {
# x, below, is a list of probabilities under a specific hypothesis
#Likelihood function is a zero for all hypotheses incompatible with the outcome
#and 1 otherwise
likelihood <- function(this_hyp,this_choice,this_outcome) {
prob_this_outcome <- (this_hyp[this_choice]*this_outcome) + (1-this_hyp[this_choice]*(1-this_outcome))
}
likes <- sapply(hyps,function(h) {likelihood(h,choice,outcome)})
unnp <- likes*prior
z <- sum(unnp)
unnp/z
}
post <- update_probs(prior,choice=1,outcome=0)
print(post)
cat_entropy(post)
log(24)
log2(24)
# First we want the probability of each outcome in each hyp
# Then we take the weighted sum over hypothesis probabilities
expected_outcome <- function(i,ph) {
sum(hyps[i])*ph
}
print(expected_outcome(2,post))
# First we want the probability of each outcome in each hyp
# Then we take the weighted sum over hypothesis probabilities
expected_outcome <- function(i,ph) {
sum(sapply(hyps, function(hyp) {hyp[i]})*ph) # the sapply results in a vector of 24. sapply loops over column and applies a function to each element
}
print(expected_outcome(2,post))
expected_entropy <- function(choice,p_hyps) {
pwin <- expected_outcome(choice,p_hyps)
cat_entropy(pwin)
}
print(expected_entropy(2,post))
#For steph 2
library(igraph)
library(tidyverse)
nodes<-c('Preference','Character','Closer','Knowledge','Visible','Choice')
graph<-matrix(c(0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,0), ncol = 6, byrow=T)
G<-graph.adjacency(graph)
View(G)
V(G)$label<-V(G)$name<-nodes
V(G)$size <- 50
V(G)$color <- 'white'
E(G)$color <- 'black'
E(G)$width <- 2
plot(G)
?igraph
V(G)$color <- 'blue'
E(G)$color <- 'black'
E(G)$width <- 2
plot(G)
V(G)$color <- 'white'
E(G)$color <- 'red'
E(G)$width <- 2
plot(G)
#I figured its easier to think about in the form of a data frame
#So we have a column indexing the state for each cause using a factor (0,1)
#And a column for the probabilities for each unique combination
pChoice<-data.frame(expand.grid(list(Preference=c(0,1),
Character=c(0,1),
Closer=c(0,1),
Knowledge=c(0,1),
Visible = c(0,1)))) %>%
mutate(Preference = factor(Preference, levels = c(0,1),
labels = c('Absent','Hotdog')),
Character = factor(Character, levels = c(0,1),
labels = c('Lazy','Sporty')),
Closer = factor(Closer, levels = c(0,1),
labels = c('Pizza','Hotdog')),
Knowledge = factor(Knowledge, levels = c(0,1),
labels = c('No','Yes')),
Visible = factor(Visible, levels = c(0,1),
labels = c('Pizza','Hotdog')),
p_choose_hotdog = NA)
View(pChoice)
head(pChoice)
#Default preference for hotdog
baserate<-.5
#Strength of preference, strength of character
strengths<-list(preference=.5, character=.5)
#preference and character match only promote hotdog choice so long as the person either
#(a) knows the area or (b) can see the hotdog
tmp1<- pChoice$Knowledge=='Yes' | pChoice$Visible=='Hotdog'
#character match only promotes pizza choice so long as the person
#(a) knows the area or (b) can see the hotdog
tmp2<-pChoice$Knowledge=='Yes' | pChoice$Visible=='Pizza'
pChoice$p_choose_hotdog<-(1-(1-baserate) *
(1-strengths[['preference']]*as.numeric(pChoice$Preference=='Hotdog')*tmp1) *
#preference for hotdog pushes toward hotdog
(1-strengths[['character']]* as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Hotdog' |
#character match to distance to hotdog pushes toward hotdog
pChoice$Character=='Sporty' & pChoice$Closer=='Pizza')*tmp1)) *
#end of the noisy OR of generative causes
(1-strengths[['character']] * as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Pizza' |
#character match to distance to pizza pushes toward pizza
pChoice$Character=='Sporty' & pChoice$Closer=='Hotdog') * tmp2)
View(pChoice)
#Here's how it looks
pChoice
#Anc here's an example of using it to maker a basic counterfactual explanation type model
#Let's suppose a person chooses pizza in situation 15
case<-cbind(pChoice[15,], Choice='Pizza')
case
#Was it reasonably to be expected given the situation (in this case yes)
p_actual<-1-case$p_choose_hotdog
?unlist
#What features of the situation contributed most to its being selected?
#counterfactual contrasts:
p_counterfactual<-1-unlist(c(pChoice %>% filter(Preference!=case$Preference,
Character==case$Character,
Closer ==case$Closer,
Knowledge==case$Knowledge,
Visible==case$Visible) %>% select(preference=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character!=case$Character,
Closer ==case$Closer,
Knowledge==case$Knowledge,
Visible==case$Visible) %>% select(character=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character==case$Character,
Closer !=case$Closer,
Knowledge==case$Knowledge,
Visible==case$Visible) %>% select(closer=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character==case$Character,
Closer ==case$Closer,
Knowledge!=case$Knowledge,
Visible==case$Visible) %>% select(knowledge=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character==case$Character,
Closer ==case$Closer,
Knowledge==case$Knowledge,
Visible!=case$Visible) %>% select(visible=p_choose_hotdog)))
View(case)
dependence<-p_actual-p_counterfactual
#In this case we might reasonably blame her lazy character or the fact that the pizza was closer
Collapse
View(case)
View(pChoice)
dependence
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_push * char_push
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
pref_match_hotdog <- as.numeric(pChoice$Preference=='Hotdog')
char_match_hotdog <- as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Hotdog' |
pChoice$Character=='Sporty' & pChoice$Closer=='Pizza')
char_match_pizza <- as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Pizza' |
#character match to distance to pizza pushes toward pizza
pChoice$Character=='Sporty' & pChoice$Closer=='Hotdog')
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
#preference and character match only promote hotdog choice so long as the person either
#(a) knows the area or (b) can see the hotdog ie generative ie things that make you want hotdog
prom_hotdog <- pChoice$Knowledge=='Yes' | pChoice$Visible=='Hotdog'
#character match only promotes pizza choice so long as the person
#(a) knows the area or (b) can see the hotdog ie preventative factor ie things that amke you want pizza
prom_pizza <- pChoice$Knowledge=='Yes' | pChoice$Visible=='Pizza'
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_push * char_push
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(strengths[['character']], strengths[['preference']], baserate))
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate))
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate))
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate)))
# My meddling
hotdog_promotion <- function() {
pref_prom_hotdog <- 1 - strengths[['preference']] * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - strengths[['character']] * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion())
# My meddling
hotdog_promotion <- function() {
pref_prom_hotdog <- 1 - strengths[['preference']] * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - strengths[['character']] * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1- strengths[['character']] *char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion())
22/110
2.002/10.10
5.005/10.01
2.005/10.1
2.002/10.1
52/110
exp(100)
exp(1)
runif(4)
sessionInfo()
sessionInfo()
sessionInfo()
install.packages("afex")
install.packages("faux")
library(broom, tidyverse, faux, afex)
library(broom)
library(faux)
library(afex)
R.version()
R.Version()
.libPaths()
.libPaths()
R.Version()
# ------- Prelims -----------
library(tidyverse)
library(ggplot2)
# ----------- Define an example prior df -------------------------
# Here define two causal vars and an exogenous noise variable for each (i.e. var epsilon A goes with A)
# in the exp setting this is 0.5
p_A <- c(.1,.9) # ie A usually has value 1... base rate for cause
p_epsA <- c(.7,.3) #... most of the time the noise var for a doesn't occur. for a to work it needs a and exp a. a is usually present but ogten doesnt work cos of noise term not working
p_B <- c(.8,.2) # B rarely fires 1...
p_epsB <- c(.3,.7) # but when it does it is strong
# And wrap them into a df called prior. Later the function should take dfs of this format:
# i.e. any number of causes as the rows, and the probs of them taking 0 and 1 as cols
params <- data.frame(rbind(p_A, p_epsA, p_B, p_epsB))
colnames(params) <- c(0,1)
# Other values set outside for now
N_cf <- 1000L # How many counterfactual samples to draw
s <- .7 # Stability
n_causes <- nrow(params)
causes <- rownames(params)
# Make a df of all combinations of variable settings
df <- expand.grid(rep(list(c(0,1)),n_causes), KEEP.OUT.ATTRS = F)
# ... with variables as the column names
colnames(df) <- causes
worlds <- nrow(df)
View(df)
structure <- 'disjunctive'
if (structure=="disjunctive") {
df$E <- as.numeric((df[1] & df[2]) | (df[3] & df[4]))
}
# Can replace with this - if rename - it is deterministic - literally gives specific outcome for set 3 causes, needs actual input. mechanical tell syou whether effects occurred given setting
# df$effect <- max( c(min(c1,e1), min(c2,e2), min(c3, e3), min(c2*c3, e23))) # BUT SAME PROBLEM - HOW TO AUTOMATICALLY DEAL WITH ANY NUMBER OF CAUSES?
mat <- as.matrix(df[,1:4])
View(mat)
# df2 <- as.matrix(df, dimnames=NULL)
# dimnames = list(c(1:16), c(causes))
# Replace every cell with the relevant indexed edge strength from params
for (k in 1:worlds){
for (cause in causes) {
a <- params[cause,df[k,cause]+1] # It needs the '+1' because r indexes from 1 not 0
mat[k,cause] <- a # ((then sometimes #*df[k,cause] if do at same time as structure but change later if need))
}
}
View(mat)
View(params)
# For each row of df, the prior is now the product of the same row of df2
df$Pr <- apply(mat, 1, prod) # parameter of the model
sum(df$Pr)
# Then loop to calculate cfs and assign causal responsibility
# Loop through possible world settings
for (c_ix in 1:worlds)
getwd()
setwd("~/Documents/GitHub/gw/Later_rating25")
# Previously this project was called 'ug data rerun' so that name crops up sometimes
library(tidyverse)
library(lme4)
# This is the data 1.5MB saved directly from testable. Also exists in wide format
df <- read.csv('rerundata.csv')
# Get just the responses and condition tag
df <- df %>% filter(trialText!="")
df <- df %>%
mutate(tag_digits = substr(as.character(tag), 2, 7)) %>%
separate(tag_digits, into = paste0("digit", 1:6), sep = 1:5) %>%
mutate(across(starts_with("digit"), as.numeric)) # Think if they are numeric then don't need to recode?
# A full version for regression
df2 <- df %>% select(mindsCode, response, digit1:digit6)
# Get the ratings: this is the intersection of two raters, taken in processing_ratings.R and merge_rating.R
ratings <- read.csv('ratings.csv')
ratings <- ratings[,-1]
# Merge with ratings
forreg <- cbind(df2, ratings)
forreg <- forreg[,-9]
run_glmers <- function(data, input_cols, output_cols, participant_col, family = binomial()) {
results <- list()
for (out_col in output_cols) {
# Subset to rows with non-missing outcome
df_sub <- data[!is.na(data[[out_col]]), c(input_cols, participant_col, out_col)]
# Build formula string
formula_str <- paste0(out_col, " ~ ", paste(input_cols, collapse = " + "), " + (1 | ", participant_col, ")")
# Convert to formula
formula <- as.formula(formula_str)
# Fit glmer
fit <- lme4::glmer(formula, data = df_sub, family = family)
results[[out_col]] <- fit
}
return(results)
}
input_cols <- c("digit1", "digit2", "digit3", "digit4", "digit5", "digit6")
output_cols <- c("a", "b", "c", "d", "e", "f", "g", "h", "unc")
participant_col <- "mindsCode"
glmer_results <- run_glmers(forreg, input_cols, output_cols, participant_col) # converegence warnings
install.packages('broom.mixed')
library(broom.mixed)
tidy_results <- lapply(glmer_results, broom.mixed::tidy)
print(tidy_results)
View(forreg)
print(summary(glmer_results[['a']]))
count(unique(data$mindsCode)
count(unique(data$mindsCode))
n <- unique(data$mindsCode)
n <- unique(df$mindsCode)
df %>% group_by(mindsCode) %>% summarise(n=n())
test <- df %>% group_by(mindsCode) %>% summarise(n=n())
View(test)
test2 <- df %>% group_by(batchID) %>% summarise(n=n())
View(test2)
df$mindsCode[df$filename == "522931_231102_214758_M043484.csv"] <- "M121212"
df$mindsCode[df$filename == "522931_231109_080020_M481805.csv"] <- "M343434"
df$mindsCode[df$filename == "522931_231103_095258_M701443.csv"] <- "M565656"
df <- df %>%
mutate(tag_digits = substr(as.character(tag), 2, 7)) %>%
separate(tag_digits, into = paste0("digit", 1:6), sep = 1:5) %>%
mutate(across(starts_with("digit"), as.numeric)) # Think if they are numeric then don't need to recode?
# A full version for regression.
df2 <- df %>% select(mindsCode, response, digit1:digit6)
# Get the ratings: this is the intersection of two raters, taken in processing_ratings.R and merge_rating.R
ratings <- read.csv('ratings.csv')
ratings <- ratings[,-1]
# Merge with ratings
forreg <- cbind(df2, ratings)
forreg <- forreg[,-9]
length(unique(df$mindsCode))
View(forreg)
View(forreg)
# Rename the columns to be readable
sixvars <- c('P', 'K', 'C', 'S', 'D', 'P')
ninelabels <- c('rated_P', 'rated_K', 'rated_C', 'rated_S', 'rated_Pu', 'rated_Ku', 'rated_Cu', 'rated_Su', 'rated_unc')
colnames(forreg)[3:8] <- sixvars
colnames(forreg)[9:17] <- ninelabels
View(forreg)
# This is the data 1.5MB saved directly from testable. Also exists in wide format
df <- read.csv('rerundata.csv')
# Get just the responses and condition tag
df <- df %>% filter(trialText!="")
df$mindsCode[df$filename == "522931_231102_214758_M043484.csv"] <- "M121212"
df$mindsCode[df$filename == "522931_231109_080020_M481805.csv"] <- "M343434"
df$mindsCode[df$filename == "522931_231103_095258_M701443.csv"] <- "M565656"
df <- df %>%
mutate(tag_digits = substr(as.character(tag), 2, 7)) %>%
separate(tag_digits, into = paste0("digit", 1:6), sep = 1:5) %>%
mutate(across(starts_with("digit"), as.numeric)) # Think if they are numeric then don't need to recode?
# A full version for regression.
df2 <- df %>% select(mindsCode, response, digit1:digit6)
# Get the ratings: this is the intersection of two raters, taken in processing_ratings.R and merge_rating.R
ratings <- read.csv('ratings.csv')
ratings <- ratings[,-1]
# Merge with ratings
forreg <- cbind(df2, ratings)
forreg <- forreg[,-9]
# Rename the columns to be readable
sixvars <- c('P', 'K', 'C', 'S', 'Dest', 'Path')
ninelabels <- c('rated_P', 'rated_K', 'rated_C', 'rated_S', 'rated_Pu', 'rated_Ku', 'rated_Cu', 'rated_Su', 'rated_unc')
colnames(forreg)[3:8] <- sixvars
colnames(forreg)[9:17] <- ninelabels
length(unique(forreg$mindsCode))
View(forreg)
# ---------- Section XX to remove 3 duplicate participants -----------------
duplicates <- c("M121212", "M343434", "M565656")
cleaned_combined <- forreg %>% filter(!(mindsCode %in% duplicates))
View(forreg)
write.csv(forreg, 'gw_rated.csv')
setwd("~/Documents/GitHub/gw/Exp2Explanation/Experiment")
# This is the data 1.5MB saved directly from testable. Also exists in wide format
df <- read.csv('rerundata.csv')
View(df)
demogdata <- data %>% filter(responseType %in% c('dropdown', 'box'))
# Previously this project was called 'ug data rerun' so that name crops up sometimes
library(tidyverse)
demogdata <- data %>% filter(responseType %in% c('dropdown', 'box'))
str(data$responseType)
# This is the data 1.5MB saved directly from testable. Also exists in wide format
data <- read.csv('rerundata.csv') # 2838 of 47
demogdata <- data %>% filter(responseType %in% c('dropdown', 'box'))
View(demogdata)
# Get just the responses lines - can also get a separate file for demogs, do that now too
df <- data %>%
filter(trialText!="") %>%
select(mindsCode, tag, response)
df <- df[!duplicated(df[c("mindsCode", "tag")]), ]
df %>% group_by(mindsCode) %>% summarise(n=n())
test <- df %>% group_by(mindsCode) %>% summarise(n=n())
View(test)
View(data)
# Get just the responses lines - 2064 - and filter for relevant info which is just subjectID, condition tag, and the reponse
df <- data %>%
filter(trialText!="") %>%
select(mindsCode, tag, response)
# Remove duplicates
df %>%
group_by(mindsCode) %>%
filter(row_number() <= 8) %>%
ungroup()
# Remove duplicates
df <- df %>%
group_by(mindsCode) %>%
filter(row_number() <= 8) %>%
ungroup()
View(df)
test <- df %>% group_by(mindsCode) %>% summarise(n=n())
View(test)
View(df)
demogdata %>% filter(mindsCode %in% c('M043484'))
demogdata %>% filter(mindsCode %in% c('M043484', 'M481805', 'M701443'))
View(demogdata)
# But we also need to remove from the demogs!
demogdata <- demogdata %>%
group_by(mindsCode) %>%
filter(row_number() <= 2) %>%
ungroup()
df <- df %>%
mutate(tag_digits = substr(as.character(tag), 2, 7)) %>%
separate(tag_digits, into = paste0("digit", 1:6), sep = 1:5) %>%
mutate(across(starts_with("digit"), as.numeric)) # Think if they are numeric then don't need to recode?
# Get just the responses lines - 2064 - and filter for relevant info which is just subjectID, condition tag, and the reponse
df <- data %>%
filter(trialText!="") %>%
select(mindsCode, tag, response)
# There is a mistake in the data where 3 participants completed it twice,
# giving good faith answers on two different occasions. Decided to remove second attempts
# Remove duplicates - 2040
df <- df %>%
group_by(mindsCode) %>%
filter(row_number() <= 8) %>%
ungroup()
df <- df %>%
mutate(tag_digits = substr(as.character(tag), 2, 7)) %>%
separate(tag_digits, into = paste0("digit", 1:6), sep = 1:5) %>%
mutate(across(starts_with("digit"), as.numeric)) %>% # Think if they are numeric then don't need to recode?
rename_with(~ letters[1:6], starts_with("digit"))
View(demogdata)
sex <- demogdata %>%
filter(responseType=='dropdown')
sex <- demogdata %>%
filter(responseType=='dropdown') %>%
group_by(response) %>%
summarise(n=n())
View(sex)
# Get age
age <- demogdata %>%
filter(responseType=='box')
# Get stats
print(mean(age$response))
View(age)
str(age$response)
age$response <- as.numeric(age$response)
# Get stats
print(mean(age$response))
print(min(age$response))
print(max(age$response))
print(sd(age$response))
write.csv(df, 'processedData.csv')
rm(list = ls())
# Get the ppt data
read.csv('../Exp2Explanation/Experiment/processedData.csv')
setwd("~/Documents/GitHub/gw/Later_rating25")
# Get the ppt data
read.csv('../Exp2Explanation/Experiment/processedData.csv')
# Get the ppt data
df <- read.csv('../Exp2Explanation/Experiment/processedData.csv')
View(df)
# Get the ratings: this is the intersection of two raters, taken in processing_ratings.R and merge_rating.R
ratings <- read.csv('ratings.csv')
View(ratings)
ratings <- ratings[,-1]
View(df)
View(ratings)
